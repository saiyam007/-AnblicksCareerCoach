"""
AI Career Advisor Service Layer.

Business logic for AI-powered career advisor features:
- Question generation
- Roadmap generation
- Profile summary generation
"""

from typing import Dict, List
from fastapi import HTTPException, status

from ..core.llmConnector import BedrockLLMClient
from ..utils.errorHandler import get_logger

logger = get_logger(__name__)


class AIService:
    """Service class for AI Career Advisor business logic."""
    
    def __init__(self):
        """Initialize AI service with LLM client."""
        try:
            self.llm_client = BedrockLLMClient()
            logger.info(" AI Service initialized with Bedrock LLM client")
        except Exception as e:
            logger.error(f" Failed to initialize AI Service: {e}")
            self.llm_client = None
    
    def _check_client_availability(self):
        """Check if LLM client is available."""
        if not self.llm_client:
            logger.error("Bedrock LLM client not initialized")
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="AI service not available. Please check configuration."
            )
    
    # ============================================================================
    # Question Generation
    # ============================================================================
    
    def generate_career_questions(self, profile_data: Dict, max_questions: int = 15) -> Dict:
        """
        Generate career-specific questions using AI.
        
        Args:
            profile_data: User profile data dictionary
            max_questions: Maximum number of questions to generate
            
        Returns:
            Dictionary containing generated questions
            
        Raises:
            HTTPException: If AI service fails
        """
        self._check_client_availability()
        
        try:
            logger.info(f"Generating {max_questions} questions for user: {profile_data.get('email')}")
            
            # Call LLM client
            questions = self.llm_client.generate_questions(profile_data, max_questions=max_questions)
            
            # Validate and normalize response
            if not questions:
                raise ValueError("Empty response from AI service")
            
            # Handle different response formats from LLM
            if isinstance(questions, dict):
                if "questions" in questions:
                    # Standard format: {"questions": [...]}
                    questions_list = questions["questions"]
                elif isinstance(questions, list):
                    # Direct list format: [...]
                    questions_list = questions
                    questions = {"questions": questions_list}
                else:
                    raise ValueError("Unexpected response format from AI service")
            elif isinstance(questions, list):
                # Direct list format: [...]
                questions_list = questions
                questions = {"questions": questions_list}
            else:
                raise ValueError("Invalid response type from AI service")
            
            # Validate questions structure
            if not isinstance(questions_list, list) or len(questions_list) == 0:
                raise ValueError("No questions generated by AI service")
            
            # Validate each question has required fields
            for i, q in enumerate(questions_list):
                if not isinstance(q, dict) or "text" not in q:
                    raise ValueError(f"Question {i+1} missing required 'text' field")
                if "id" not in q:
                    q["id"] = f"q{i+1}"  # Auto-generate ID if missing
            
            logger.info(f" Generated {len(questions_list)} questions")
            
            return questions
            
        except RuntimeError as e:
            logger.error(f"Runtime error generating questions: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Failed to generate questions: {str(e)}"
            )
        except ValueError as e:
            logger.error(f"Validation error: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="AI service returned invalid response"
            )
        except Exception as e:
            logger.exception("Unexpected error generating questions")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"An unexpected error occurred: {str(e)}"
            )
    
    # ============================================================================
    # Roadmap Generation
    # ============================================================================
    
    def generate_career_roadmap(self, profile_data: Dict, answers_data: List[Dict]) -> Dict:
        """
        Generate personalized career roadmap using AI.
        
        Args:
            profile_data: User profile data dictionary
            answers_data: List of user answers to questions
            
        Returns:
            Dictionary containing career paths
            
        Raises:
            HTTPException: If AI service fails
        """
        self._check_client_availability()
        
        try:
            logger.info(f"Generating roadmap for user: {profile_data.get('email')} with {len(answers_data)} answers")
            
            # Call LLM client
            roadmap = self.llm_client.generate_roadmap(profile_data, answers_data)
            
            # Validate response
            if not roadmap or "careerPaths" not in roadmap:
                raise ValueError("Invalid response from AI service")
            
            logger.info(f" Generated {len(roadmap.get('careerPaths', []))} career paths")
            
            return roadmap
            
        except RuntimeError as e:
            logger.error(f"Runtime error generating roadmap: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Failed to generate roadmap: {str(e)}"
            )
        except ValueError as e:
            logger.error(f"Validation error: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="AI service returned invalid response"
            )
        except Exception as e:
            logger.exception("Unexpected error generating roadmap")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"An unexpected error occurred: {str(e)}"
            )
    
    # ============================================================================
    # Profile Summary Generation
    # ============================================================================
    
    def generate_profile_summary(self, profile_data: Dict) -> str:
        """
        Generate profile summary using AI.
        
        Args:
            profile_data: User profile data dictionary
            
        Returns:
            Generated profile summary text
            
        Raises:
            HTTPException: If AI service fails
        """
        self._check_client_availability()
        
        try:
            logger.info(f"Generating profile summary for user: {profile_data.get('email')}")
            
            # Call LLM client
            summary_obj = self.llm_client.generate_profile_summary(profile_data)
            
            # Validate response
            if not summary_obj or "summary" not in summary_obj:
                raise ValueError("Invalid response from AI service")
            
            profile_summary_text = summary_obj["summary"]
            
            # Sanitize text to remove problematic characters
            profile_summary_text = self._sanitize_summary_text(profile_summary_text)
            
            logger.info(f" Generated profile summary for {profile_data.get('email')}")
            
            return profile_summary_text
            
        except RuntimeError as e:
            logger.error(f"Runtime error generating profile summary: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Failed to generate profile summary: {str(e)}"
            )
        except ValueError as e:
            logger.error(f"Validation error: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="AI service returned invalid response"
            )
        except Exception as e:
            logger.exception("Unexpected error generating profile summary")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"An unexpected error occurred: {str(e)}"
            )
    
    # ============================================================================
    # Helper Methods
    # ============================================================================
    
    def _sanitize_summary_text(self, text: str) -> str:
        """
        Sanitize profile summary text.
        
        Args:
            text: Raw summary text
            
        Returns:
            Sanitized summary text
        """
        if not text:
            return ""
        
        # Remove problematic characters for JSON/curl
        sanitized = (
            text
            .replace(""", '"')
            .replace(""", '"')
            .replace("'", "'")
            .replace("'", "'")
            .replace("'", "")        # Remove single quotes entirely
            .replace("\n", " ")      # Remove newlines
            .strip()
        )
        
        return sanitized
    
    def check_health(self) -> Dict:
        """
        Check AI service health status.
        
        Returns:
            Health status dictionary
        """
        from ..utils.errorHandler import settings
        
        return {
            "service": "AI Career Advisor",
            "status": "healthy" if self.llm_client else "unavailable",
            "llm_client_initialized": self.llm_client is not None,
            "configuration": {
                "bedrock_model_id": settings.BEDROCK_MODEL_ID or "(not set)",
                "bedrock_region": settings.BEDROCK_REGION,
                "dry_run_mode": settings.DRY_RUN
            },
            "endpoints": {
                "generate_questions": "/ai/profile/questions",
                "generate_roadmap": "/ai/profile/roadmap",
                "generate_summary": "/ai/profile/summary"
            }
        }


# ============================================================================
# Dependency Injection
# ============================================================================

# Global service instance
_ai_service = None

def get_ai_service() -> AIService:
    """
    Get or create AI service instance (singleton pattern).
    
    Returns:
        AIService instance
    """
    global _ai_service
    if _ai_service is None:
        _ai_service = AIService()
    return _ai_service

